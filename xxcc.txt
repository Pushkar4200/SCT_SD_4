# working_scraper.py
"""
E-Commerce Web Scraper with Verified Working Websites
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import time
from urllib.parse import urljoin, urlparse

class WorkingEcommerceScraper:
    def _init_(self):
        """Initialize the scraper with default headers"""
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        self.session.headers.update(self.headers)
        
        # List of verified working websites
        self.working_sites = {
            '1': {
                'name': 'WebScraper.io Test Site',
                'url': 'https://webscraper.io/test-sites/e-commerce/allinone',
                'description': 'Best for testing - specifically made for web scraping practice'
            },
            '2': {
                'name': 'Books to Scrape',
                'url': 'http://books.toscrape.com/',
                'description': 'Book store practice site - very reliable'
            },
            '3': {
                'name': 'Quotes to Scrape',
                'url': 'http://quotes.toscrape.com/',
                'description': 'Quotes website - always works'
            },
            '4': {
                'name': 'Demo Blaze Store',
                'url': 'https://www.demoblaze.com/',
                'description': 'Demo e-commerce site for testing'
            }
        }
    
    def display_working_sites(self):
        """Display list of verified working websites"""
        print("\n✅ VERIFIED WORKING WEBSITES:")
        print("=" * 60)
        for key, site in self.working_sites.items():
            print(f"{key}. {site['name']}")
            print(f"   📍 {site['url']}")
            print(f"   💡 {site['description']}")
            print()
    
    def is_valid_url(self, url):
        """Check if the provided URL is valid"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False
    
    def get_website_content(self, url):
        """Fetch the website content with error handling"""
        try:
            print(f"🔍 Connecting to: {url}")
            response = self.session.get(url, timeout=20)
            response.raise_for_status()
            
            # Check if content is HTML
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' not in content_type:
                print("❌ The URL does not contain HTML content")
                return None
            
            print("✅ Successfully retrieved website content")
            return response.text
            
        except requests.exceptions.ConnectTimeout:
            print("❌ Connection timeout. The website might be down or unreachable.")
            return None
        except requests.exceptions.ConnectionError:
            print("❌ Connection error. Please check your internet connection and the website URL.")
            return None
        except requests.exceptions.HTTPError as e:
            print(f"❌ HTTP Error: {e}")
            return None
        except requests.exceptions.RequestException as e:
            print(f"❌ Error fetching the website: {e}")
            return None
    
    def scrape_webscraper_io(self, html_content, base_url):
        """Specific scraper for webscraper.io test site"""
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        # WebScraper.io specific selectors
        product_containers = soup.find_all('div', class_='thumbnail')
        
        for product in product_containers:
            try:
                # Product name
                name_elem = product.find('a', class_='title')
                name = name_elem.text.strip() if name_elem else 'N/A'
                
                # Price
                price_elem = product.find('h4', class_='price')
                price = price_elem.text.strip() if price_elem else 'N/A'
                
                # Description
                desc_elem = product.find('p', class_='description')
                description = desc_elem.text.strip() if desc_elem else 'N/A'
                
                # Rating (count stars)
                rating_elem = product.find('div', class_='ratings')
                if rating_elem:
                    stars = rating_elem.find_all('span', class_='glyphicon-star')
                    rating = len(stars)
                else:
                    rating = 0
                
                # Reviews
                reviews_elem = product.find('p', class_='review-count')
                reviews = reviews_elem.text.strip() if reviews_elem else '0 reviews'
                
                products.append({
                    'name': name,
                    'price': price,
                    'rating': f"{rating}/5 stars",
                    'reviews': reviews,
                    'description': description,
                    'url': base_url
                })
                
            except Exception as e:
                continue
        
        return products
    
    def scrape_books_toscrape(self, html_content, base_url):
        """Specific scraper for books.toscrape.com"""
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        # Books to Scrape specific selectors
        book_containers = soup.find_all('article', class_='product_pod')
        
        for book in book_containers:
            try:
                # Book title
                title_elem = book.find('h3').find('a')
                title = title_elem['title'] if title_elem and 'title' in title_elem.attrs else 'N/A'
                
                # Price
                price_elem = book.find('p', class_='price_color')
                price = price_elem.text.strip() if price_elem else 'N/A'
                
                # Rating
                rating_elem = book.find('p', class_='star-rating')
                if rating_elem:
                    rating_class = rating_elem.get('class', [])
                    if len(rating_class) > 1:
                        rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}
                        rating = rating_map.get(rating_class[1], 'N/A')
                    else:
                        rating = 'N/A'
                else:
                    rating = 'N/A'
                
                # Availability
                availability_elem = book.find('p', class_='instock')
                availability = availability_elem.text.strip() if availability_elem else 'N/A'
                
                # URL
                link_elem = book.find('h3').find('a')
                book_url = urljoin(base_url, link_elem['href']) if link_elem and 'href' in link_elem.attrs else base_url
                
                products.append({
                    'name': title,
                    'price': price,
                    'rating': f"{rating}/5" if rating != 'N/A' else 'N/A',
                    'availability': availability,
                    'category': 'Books',
                    'url': book_url
                })
                
            except Exception as e:
                continue
        
        return products
    
    def scrape_quotes_toscrape(self, html_content, base_url):
        """Specific scraper for quotes.toscrape.com"""
        soup = BeautifulSoup(html_content, 'html.parser')
        quotes = []
        
        quote_containers = soup.find_all('div', class_='quote')
        
        for quote in quote_containers:
            try:
                # Quote text
                text_elem = quote.find('span', class_='text')
                text = text_elem.text.strip() if text_elem else 'N/A'
                
                # Author
                author_elem = quote.find('small', class_='author')
                author = author_elem.text.strip() if author_elem else 'N/A'
                
                # Tags
                tags_elems = quote.find_all('a', class_='tag')
                tags = ', '.join(tag.text.strip() for tag in tags_elems) if tags_elems else 'N/A'
                
                quotes.append({
                    'quote': text,
                    'author': author,
                    'tags': tags,
                    'url': base_url
                })
                
            except Exception as e:
                continue
        
        return quotes
    
    def scrape_generic_site(self, html_content, base_url):
        """Generic scraper for other websites"""
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        # Try multiple common product selectors
        selectors = [
            '.product', '.item', '.card', '[data-product]',
            '.thumbnail', '.product-item', '.product-listing'
        ]
        
        for selector in selectors:
            containers = soup.select(selector)
            if containers:
                print(f"✅ Found {len(containers)} items with selector: {selector}")
                
                for container in containers[:15]:  # Limit to first 15 items
                    try:
                        # Try to extract name from various elements
                        name = 'N/A'
                        name_selectors = ['h1', 'h2', 'h3', 'h4', '.title', '.name', '.product-name']
                        for ns in name_selectors:
                            name_elem = container.find(ns)
                            if name_elem:
                                name = name_elem.get_text(strip=True)
                                if name and len(name) > 3:  # Valid name
                                    break
                        
                        # Try to extract price
                        price = 'N/A'
                        price_text = container.get_text()
                        price_match = re.search(r'[\$£€¥]\s*(\d+[.,]\d+|\d+)', price_text)
                        if price_match:
                            price = price_match.group(0)
                        
                        if name != 'N/A':
                            products.append({
                                'name': name,
                                'price': price,
                                'rating': 'N/A',
                                'url': base_url
                            })
                            
                    except Exception:
                        continue
                
                if products:
                    break
        
        return products
    
    def extract_data(self, html_content, url):
        """Extract data based on the website"""
        if 'webscraper.io' in url:
            return self.scrape_webscraper_io(html_content, url), 'products'
        elif 'books.toscrape.com' in url:
            return self.scrape_books_toscrape(html_content, url), 'products'
        elif 'quotes.toscrape.com' in url:
            return self.scrape_quotes_toscrape(html_content, url), 'quotes'
        else:
            return self.scrape_generic_site(html_content, url), 'products'
    
    def save_to_csv(self, data, filename, data_type='products'):
        """Save data to CSV file"""
        if not data:
            print("❌ No data found to save")
            return False
        
        try:
            df = pd.DataFrame(data)
            df.to_csv(filename, index=False, encoding='utf-8')
            print(f"✅ Successfully saved {len(data)} {data_type} to {filename}")
            return True
        except Exception as e:
            print(f"❌ Error saving to CSV: {e}")
            return False
    
    def display_results(self, data, data_type='products'):
        """Display the scraped results"""
        if not data:
            print("❌ No data found to display")
            return
        
        print(f"\n📊 SCRAPING RESULTS ({data_type.upper()}):")
        print("=" * 80)
        print(f"Total {data_type} found: {len(data)}")
        print("=" * 80)
        
        for i, item in enumerate(data[:8], 1):  # Show first 8 items
            if data_type == 'products':
                print(f"{i}. {item['name']}")
                print(f"   💰 Price: {item['price']}")
                print(f"   ⭐ Rating: {item.get('rating', 'N/A')}")
                if 'reviews' in item:
                    print(f"   📊 Reviews: {item['reviews']}")
            elif data_type == 'quotes':
                print(f"{i}. {item['quote']}")
                print(f"   ✍  Author: {item['author']}")
                print(f"   🏷  Tags: {item['tags']}")
            print("-" * 80)

def main():
    """Main function to run the web scraper"""
    scraper = WorkingEcommerceScraper()
    
    print("🛒 E-COMMERCE WEB SCRAPER")
    print("=" * 60)
    
    # Show working sites
    scraper.display_working_sites()
    
    # Get URL from user
    while True:
        choice = input("Choose an option:\n1. Use a recommended website\n2. Enter custom URL\nEnter choice (1 or 2): ").strip()
        
        if choice == '1':
            site_choice = input("Enter the number of the website to scrape (1-4): ").strip()
            if site_choice in scraper.working_sites:
                url = scraper.working_sites[site_choice]['url']
                break
            else:
                print("❌ Invalid choice. Please enter 1, 2, 3, or 4")
        elif choice == '2':
            url = input("🌐 Enter the website URL to scrape: ").strip()
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            break
        else:
            print("❌ Please enter 1 or 2")
    
    # Validate URL
    if not scraper.is_valid_url(url):
        print("❌ Invalid URL format.")
        return
    
    # Fetch website content
    html_content = scraper.get_website_content(url)
    if not html_content:
        print(f"💡 Tip: Try one of the verified working websites above!")
        return
    
    # Extract data
    print("🔄 Extracting information...")
    data, data_type = scraper.extract_data(html_content, url)
    
    if data:
        # Display results
        scraper.display_results(data, data_type)
        
        # Save to CSV
        default_name = f"{data_type}_{int(time.time())}.csv"
        filename = input(f"\n💾 Enter CSV filename (or press Enter for '{default_name}'): ").strip()
        if not filename:
            filename = default_name
        elif not filename.endswith('.csv'):
            filename += '.csv'
        
        scraper.save_to_csv(data, filename, data_type)
        
        print(f"\n🎯 Scraping completed successfully!")
        print(f"📦 {data_type.capitalize()} found: {len(data)}")
        print(f"💾 File saved as: {filename}")
        
    else:
        print("❌ No information found on this website.")
        print("💡 Try one of the verified working websites above!")

if _name_ == "_main_":
    main()